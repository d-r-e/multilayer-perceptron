{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from src.ft_math import mean, softmax, sigmoid, cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius</th>\n",
       "      <th>texture</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>area</th>\n",
       "      <th>compactness</th>\n",
       "      <th>smoothness</th>\n",
       "      <th>concavity</th>\n",
       "      <th>concave_point</th>\n",
       "      <th>...</th>\n",
       "      <th>radiusW</th>\n",
       "      <th>textureW</th>\n",
       "      <th>perimeterW</th>\n",
       "      <th>areaW</th>\n",
       "      <th>compactnessW</th>\n",
       "      <th>smoothnessW</th>\n",
       "      <th>concavityW</th>\n",
       "      <th>concave_pointsW</th>\n",
       "      <th>symmetryW</th>\n",
       "      <th>fractal_dimensionW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius  texture  perimeter    area  compactness  \\\n",
       "0      842302         M   17.99    10.38     122.80  1001.0      0.11840   \n",
       "1      842517         M   20.57    17.77     132.90  1326.0      0.08474   \n",
       "2    84300903         M   19.69    21.25     130.00  1203.0      0.10960   \n",
       "3    84348301         M   11.42    20.38      77.58   386.1      0.14250   \n",
       "4    84358402         M   20.29    14.34     135.10  1297.0      0.10030   \n",
       "..        ...       ...     ...      ...        ...     ...          ...   \n",
       "564    926424         M   21.56    22.39     142.00  1479.0      0.11100   \n",
       "565    926682         M   20.13    28.25     131.20  1261.0      0.09780   \n",
       "566    926954         M   16.60    28.08     108.30   858.1      0.08455   \n",
       "567    927241         M   20.60    29.33     140.10  1265.0      0.11780   \n",
       "568     92751         B    7.76    24.54      47.92   181.0      0.05263   \n",
       "\n",
       "     smoothness  concavity  concave_point  ...  radiusW  textureW  perimeterW  \\\n",
       "0       0.27760    0.30010        0.14710  ...   25.380     17.33      184.60   \n",
       "1       0.07864    0.08690        0.07017  ...   24.990     23.41      158.80   \n",
       "2       0.15990    0.19740        0.12790  ...   23.570     25.53      152.50   \n",
       "3       0.28390    0.24140        0.10520  ...   14.910     26.50       98.87   \n",
       "4       0.13280    0.19800        0.10430  ...   22.540     16.67      152.20   \n",
       "..          ...        ...            ...  ...      ...       ...         ...   \n",
       "564     0.11590    0.24390        0.13890  ...   25.450     26.40      166.10   \n",
       "565     0.10340    0.14400        0.09791  ...   23.690     38.25      155.00   \n",
       "566     0.10230    0.09251        0.05302  ...   18.980     34.12      126.70   \n",
       "567     0.27700    0.35140        0.15200  ...   25.740     39.42      184.60   \n",
       "568     0.04362    0.00000        0.00000  ...    9.456     30.37       59.16   \n",
       "\n",
       "      areaW  compactnessW  smoothnessW  concavityW  concave_pointsW  \\\n",
       "0    2019.0       0.16220      0.66560      0.7119           0.2654   \n",
       "1    1956.0       0.12380      0.18660      0.2416           0.1860   \n",
       "2    1709.0       0.14440      0.42450      0.4504           0.2430   \n",
       "3     567.7       0.20980      0.86630      0.6869           0.2575   \n",
       "4    1575.0       0.13740      0.20500      0.4000           0.1625   \n",
       "..      ...           ...          ...         ...              ...   \n",
       "564  2027.0       0.14100      0.21130      0.4107           0.2216   \n",
       "565  1731.0       0.11660      0.19220      0.3215           0.1628   \n",
       "566  1124.0       0.11390      0.30940      0.3403           0.1418   \n",
       "567  1821.0       0.16500      0.86810      0.9387           0.2650   \n",
       "568   268.6       0.08996      0.06444      0.0000           0.0000   \n",
       "\n",
       "     symmetryW  fractal_dimensionW  \n",
       "0       0.4601             0.11890  \n",
       "1       0.2750             0.08902  \n",
       "2       0.3613             0.08758  \n",
       "3       0.6638             0.17300  \n",
       "4       0.2364             0.07678  \n",
       "..         ...                 ...  \n",
       "564     0.2060             0.07115  \n",
       "565     0.2572             0.06637  \n",
       "566     0.2218             0.07820  \n",
       "567     0.4087             0.12400  \n",
       "568     0.2871             0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load './src/Perceptron.py'\n",
    "from src.ft_math import relu, softmax\n",
    "from src.ft_math import cross_entropy as cost\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "# https://towardsdatascience.com/building-neural-network-from-scratch-9c88535bf8e9\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    # A building block. Each layer is capable of performing two things:\n",
    "    # - Process input to get output:\n",
    "    \"\"\" output = layer.forward(input) \"\"\"\n",
    "    # - Propagate gradients through itself:\n",
    "    \"\"\" grad_input = layer.backward(input, grad_output) \"\"\"\n",
    "    # Some layers also have learnable parameters which they update during\n",
    "    # layer.backward.\n",
    "\n",
    "    def __init__(self):\n",
    "        # Here we can initialize layer parameters (if any) and auxiliary stuff.\n",
    "        # A dummy layer does nothing\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Takes input data of shape [batch, input_units],\n",
    "        # returns output data [batch, output_units]\n",
    "        # A dummy layer just returns whatever it gets as input.\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        # Performs a backpropagation step through the layer, with respect to\n",
    "        # the given input.\n",
    "        # To compute loss gradients w.r.t input, we need to apply chain\n",
    "        # rule (backprop):\n",
    "        \"\"\" d loss / d x  = (d loss / d layer) * (d layer / d x) \"\"\"\n",
    "        # Luckily, we already receive d loss / d layer as input, so you only\n",
    "        # need to multiply it by d layer / d x.\n",
    "        # If our layer has parameters (e.g. dense layer), we need to update\n",
    "        # them here using d loss / d layer\n",
    "        # The gradient of a dummy layer is just grad_output, but we'll write\n",
    "        # more explicitly\n",
    "        num_units = input.shape[1]\n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        return np.dot(grad_output, d_layer_d_input)  # chain rule\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        relu_forward = np.maximum(0,input)\n",
    "        return relu_forward\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        relu_grad = input > 0\n",
    "        return grad_output * relu_grad\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        \"\"\" A dense layer performs a learned affine transformation:\n",
    "            f(x) = <W*x> + b \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.normal(\n",
    "            loc=0.0,\n",
    "            scale=np.sqrt(2/(input_units + output_units)),\n",
    "            size=(input_units, output_units)\n",
    "            )\n",
    "        self.biases = np.zeros(output_units)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\" Perform an affine transformation:\n",
    "            f(x) = <W*x> + b \"\"\"\n",
    "\n",
    "        # input shape: [batch, input_units]\n",
    "        # output shape: [batch, output units]\n",
    "\n",
    "        return np.dot(input.T, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "\n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = np.dot(input, grad_output)\n",
    "        grad_biases = grad_output.mean(axis=0) * input.shape[0]\n",
    "\n",
    "        assert grad_weights.shape == self.weights.shape\n",
    "        assert grad_biases.shape == self.biases.shape\n",
    "\n",
    "        # Here we perform a stochastic gradient descent step to update the weights\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "def softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "    logits_for_answers = logits[np.arange(len(logits)), reference_answers]\n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits), axis=-1))\n",
    "    return xentropy\n",
    "\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits, reference_answers):\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)), reference_answers] = 1\n",
    "\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1, keepdims=True)\n",
    "\n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\" n = 8\n",
    "    l0 = Dense(n, 2)\n",
    "    X = np.eye(n)\n",
    "    print(l0.forward(X))\n",
    "    print(l0.backward(X, l0.forward(X)))\n",
    "    print() \"\"\"\n",
    "    x2 = np.array([1, 1, 1, 1, 1, 1])\n",
    "    x3 = x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(np.array(df.iloc[:,2:22])).T\n",
    "X_train = X_train.reshape([X_train.shape[0], -1])\n",
    "y_train = np.array(df['diagnosis'].map({'M':1,'B':0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "network.append(Dense(X_train.shape[0], 20))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(20, 10))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(10, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(network, X):\n",
    "    activations = []\n",
    "    input = X\n",
    "    for l in network:\n",
    "        activations.append(l.forward(input))\n",
    "        input = activations[-1]\n",
    "    assert len(activations) == len(network)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, X):\n",
    "    logits = forward(network, X)[-1]\n",
    "    return logits.argmax(axis=1)  # the index of the most probable value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, X, y):\n",
    "    layer_activations = forward(network, X)\n",
    "    layer_inputs = [X] + layer_activations\n",
    "    logits = layer_activations[-1]\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "    for layer_index in range(len(network))[::-1]:\n",
    "        layer = network[layer_index]\n",
    "        loss_grad = layer.backward(layer_inputs[layer_index], loss_grad)\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain.shape:(20, 569),y_train.shape(569,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (20,10) and (20,20) not aligned: 10 (dim 1) != 20 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-cec9b8709700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'xtrain.shape:{X_train.shape},y_train.shape{y_train.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-729e8f80b87d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(network, X, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlayer_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlayer_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_activations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_crossentropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-77b1fdebb510>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(network, X)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-51a5cfdb8c34>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# output shape: [batch, output units]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (20,10) and (20,20) not aligned: 10 (dim 1) != 20 (dim 0)"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []\n",
    "for epoch in range(25):\n",
    "    print(f'xtrain.shape:{X_train.shape},y_train.shape{y_train.shape}')\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train.T,y_train,batchsize=10,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python39064bit022e776a03e14e82a1bf794920063e93"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
